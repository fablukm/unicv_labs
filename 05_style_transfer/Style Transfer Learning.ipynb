{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer Learning\n",
    "Our final lab consists of an implementation of style transfer learning in PyTorch.\n",
    "A short recap:\n",
    "1. Load a pre-trained VGG-16 model\n",
    "2. Identify which filters correspond to style and content\n",
    "3. Modify an input image to minimize style loss and content loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Load the libraries and download model\n",
    "Apart from pytorch, we are going to load torchvision: It consists of popular datasets, model architectures, and common image transformations for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "\n",
    "import VGG\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load configuration and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"MODEL\": {\n",
    "            \"DIR\": \"./\",\n",
    "            \"FILENAME\": \"vgg_conv.pth\"\n",
    "            },\n",
    "        \"IMAGES\": {\n",
    "            \"DIR\": \"./\",\n",
    "            \"STYLE_TEMPLATE\": \"style.jpg\",\n",
    "            \"CONTENT_TEMPLATE\": \"content.jpg\",\n",
    "            \"OUTPUT\": \"out.jpg\",\n",
    "            \"SIZE\": 500\n",
    "        },\n",
    "        \"ITERATION\": {\n",
    "            \"MAX_ITER\": 1,\n",
    "            \"SHOW_ITER\": 50\n",
    "        },\n",
    "        \"LAYERS\":{\n",
    "            \"STYLE\": [\"r11\",\"r21\",\"r31\",\"r41\", \"r51\"],\n",
    "            \"CONTENT\": [\"r42\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config):\n",
    "    'load model'\n",
    "    model = VGG.VGG()\n",
    "    model_dir = config['MODEL']['DIR']\n",
    "    model_filename = config['MODEL']['FILENAME']\n",
    "    model.load_state_dict(torch.load(os.path.join(model_dir, model_filename)))\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Define auxiliary functions\n",
    "To compute content and style losses, it is required to implement functions that return Gram matrices of a set of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b, c, h, w = input.size()\n",
    "        F = input.view(b, c, h*w)\n",
    "        G = torch.bmm(F, F.transpose(1, 2))\n",
    "        G.div_(h*w)\n",
    "        return G\n",
    "\n",
    "\n",
    "class GramMSELoss(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        out = nn.MSELoss()(GramMatrix()(input), target)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Define Pre- and Postprocessing\n",
    "For **preprocessing** we need to resize the images to a desired size, transform their colour space from RGB to BGR, and normalise the images to a standard. The subtracted mean is taken from the image database *imagenet*.\n",
    "\n",
    "For **postprocessing** we need to invert the normalization and change back to the RGB colourspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_post_processing(config):\n",
    "    'Define Pre-Processing (func)'\n",
    "    preproc = transforms.Compose([transforms.Resize(config[\"IMAGES\"][\"SIZE\"]),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Lambda(\n",
    "        lambda x: x[torch.LongTensor([2, 1, 0])]),  # RGB2BGR\n",
    "        transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961],  \n",
    "                             std=[1, 1, 1]),\n",
    "        transforms.Lambda(lambda x: x.mul_(255)),\n",
    "    ])\n",
    "\n",
    "    'Define Post-Processing (func)'\n",
    "    postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
    "                                 transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961],  # add imagenet mean\n",
    "                                                      std=[1, 1, 1]),\n",
    "                                 transforms.Lambda(\n",
    "                                     lambda x: x[torch.LongTensor([2, 1, 0])]),  # BGR2RGB\n",
    "                                 ])\n",
    "\n",
    "    postpb = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "    def postproc(tensor):  # cutoff\n",
    "        t = postpa(tensor)\n",
    "        t[t > 1] = 1\n",
    "        t[t < 0] = 0\n",
    "        img = postpb(t)\n",
    "        return img\n",
    "\n",
    "    return preproc, postproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc, postproc = get_pre_post_processing(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Load images\n",
    "Now we load the images from the drive. The output image will be initialised to the content template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(config, prep):\n",
    "    '''load images, ordered as [style_image, content_image]'''\n",
    "    image_dir = config['IMAGES']['DIR']\n",
    "    image_names = [config['IMAGES']['STYLE_TEMPLATE'],\n",
    "                   config['IMAGES']['CONTENT_TEMPLATE']]\n",
    "    imgs = [Image.open(os.path.join(image_dir, name)) for name in image_names]\n",
    "    \n",
    "    # show images\n",
    "    plt.figure()\n",
    "    plt.imshow(imgs[0])\n",
    "    plt.title('Style template')\n",
    "    plt.figure()\n",
    "    plt.imshow(imgs[1])\n",
    "    plt.title('Content template')\n",
    "    \n",
    "    imgs_torch = [prep(img) for img in imgs]\n",
    "    if torch.cuda.is_available():\n",
    "        imgs_torch = [Variable(img.unsqueeze(0).cuda()) for img in imgs_torch]\n",
    "    else:\n",
    "        imgs_torch = [Variable(img.unsqueeze(0)) for img in imgs_torch]\n",
    "    style_image, content_image = imgs_torch\n",
    "\n",
    "    opt_img = Variable(content_image.data.clone(), requires_grad=True)\n",
    "    return opt_img, style_image, content_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_img, style_image, content_image = load_images(config, preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Define style and content weights\n",
    "The weights are taken from the paper Gatys et al.: Image Style Transfer Using Convolutional Neural Networks, 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights():\n",
    "    style_weights = [1e3/n**2 for n in [64, 128, 256, 512, 512]]\n",
    "    content_weights = [1e0]\n",
    "    weights = style_weights + content_weights\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Define loss function and weights\n",
    "This is a crucial step: We implement the combination between content loss and style loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(config):\n",
    "    style_layers = config['LAYERS']['STYLE']\n",
    "    content_layers = config['LAYERS']['CONTENT']\n",
    "    loss_layers = style_layers + content_layers\n",
    "    loss_fns = [GramMSELoss()] * len(style_layers) + \\\n",
    "        [nn.MSELoss()] * len(content_layers)\n",
    "    if torch.cuda.is_available():\n",
    "        loss_fns = [loss_fn.cuda() for loss_fn in loss_fns]\n",
    "    return loss_fns, loss_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fns, loss_layers = get_loss(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targets(config, style_image, content_image, model):\n",
    "    '''compute loss functions'''\n",
    "    style_layers = config['LAYERS']['STYLE']\n",
    "    content_layers = config['LAYERS']['CONTENT']\n",
    "    \n",
    "    style_targets = [GramMatrix()(A).detach()\n",
    "                     for A in model(style_image, style_layers)]\n",
    "    content_targets = [A.detach()\n",
    "                       for A in model(content_image, content_layers)]\n",
    "    \n",
    "    targets = style_targets + content_targets\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = targets(config, style_image, content_image, model)\n",
    "\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 The main functions\n",
    "Now we have gathered all ingredients necessary to define a main function for style transfer. As an optimizer, we use a Quasi-Newton method called the [Limited-Memory Broyden–Fletcher–Goldfarb–Shanno (LMBFGS)](https://en.wikipedia.org/wiki/Limited-memory_BFGS) Algorithm, again following the aforementioned paper and implementation by Leon Gatys et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config, model, opt_img, loss_layers, weights, loss_fns, targets, postproc):\n",
    "    max_iter = config[\"ITERATION\"][\"MAX_ITER\"]\n",
    "    optimizer = optim.LBFGS([opt_img])\n",
    "    n_iter = [0]\n",
    "\n",
    "    max_iter = config['ITERATION']['MAX_ITER']\n",
    "    show_iter = config['ITERATION']['SHOW_ITER']\n",
    "\n",
    "    while n_iter[0] <= max_iter:\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = model(opt_img, loss_layers)\n",
    "            layer_losses = [weights[a] * loss_fns[a]\n",
    "                            (A, targets[a]) for a, A in enumerate(out)]\n",
    "            loss = sum(layer_losses)\n",
    "            loss.backward()\n",
    "            n_iter[0] += 1\n",
    "            if n_iter[0] % show_iter == (show_iter-1):\n",
    "                print('Iteration: {}, loss: {}'.format(\n",
    "                    n_iter[0]+1, loss.item()))\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "    out_img = postproc(opt_img.data[0].cpu().squeeze())\n",
    "    out_img.save(os.path.join(config['IMAGES']['DIR'], config['IMAGES']['OUTPUT']))\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = run(config, model, opt_img, loss_layers,\n",
    "              weights, loss_fns, targets, postproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

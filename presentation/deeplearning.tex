%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Slide mode
\documentclass[10pt,hyperref={pdfpagelabels=false}]{beamer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Handout mode
%\documentclass[10pt,hyperref={pdfpagelabels=false},handout]{beamer}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages & Setup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{beamerthemeSAM}
\include{header/header_packages}

% optional
\include{header/header_letters}
\include{header/header_shortcuts}
\include{header/header_macros}
\include{header/header_setup}
\include{header/header_tricks}
\include{macros}
\usepackage{graphicx}


\usepackage{pgf,tikz}
\usetikzlibrary{arrows}
\usetikzlibrary[patterns]

\usepackage{pgfplots}
\pgfplotsset{width=7cm,compat=1.8}
\usepackage{pgfplotstable}
\pgfmathsetseed{1138} % set the random seed
\pgfplotstableset{ % Define the equations for x and y
    create on use/x/.style={create col/expr={42+2*\pgfplotstablerow}},
    create on use/y/.style={create col/expr={(0.6*\thisrow{x}+130)+5*rand}}
}
% create a new table with 30 rows and columns x and y:
\pgfplotstablenew[columns={x,y}]{30}\loadedtable

\def\layersep{2cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title Information
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title%
{Introduction to Deep Learning}

\author[ Fabian M\"uller]{Fabian M\"uller, PhD}
\institute{Ziemer Ophthalmic Systems in Bienne, Switzerland}

\date{June 11\nth 2019, University of Cape Verde}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title Page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\setbeamerfont{title}{size=\large} % instead of \Large in beamerfontthemedefault, because I have a long title
\frame[t,plain]{\titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%\mytoc[currentsection]{sec:introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{What is Machine Learning?}
    \begin{itemize}
        \item {\bf Given:} A decision task humans can do without knowing the exact decision mechanics.
        {\small
        \item Examples:
            \begin{enumerate}
                \item From a time series of measurements, identify outliers.
                \item Classifying Emails for Spam/No Spam
                \item Identifying different parts of an eye from an image.
                \item Generating images from pictures in a certain style.
            \end{enumerate}
        }
        \item {\bf Problem:} The decision ''algorithm'' our brain follows is far from understood, and therefore impossible to program.
        \item {\bf Solution:} Use an approach {\bf based on given data}. Find solutions to your task by minimizing an \emph{error function} over the data.
        \item Main paradigms: {\bf Supervised} vs {\bf unsupervised} learning.
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Example: Image Classification}
    \begin{itemize}
        \item \emph{Given:} An image taken with 4MP resolution: $3\cdot 4\cdot 10^6$ numbers.\\
        \emph{Goal:} A number $p\in[0,1]$ corresponding to the likelihood that there is a car in the picture.
        \item Deep neural networks in a nutshell:
        \begin{enumerate}
            \item \label{step:nn1}Compute linear combinations of the input numbers.
            \item \label{step:nn2}Apply non-linear functions to the results.
            \item Repeat steps \ref{step:nn1}-\ref{step:nn2} with the output of step \ref{step:nn2}.
        \end{enumerate}
        \item Coefficients in the linear combinations of step \ref{step:nn1}:
        \begin{enumerate}
            \item Each choice of coefficients yields an output $p(I)$ for each image $I$.
            \item Prepare a {\bf dataset} of $N_{\text{car}}$ images with cars ($p_{\text{exact}}(I)=1$) and $N_{\text{not car}}$ images without cars ($p_{\text{exact}}(I)=0$).
            \item Try to find a choice of coefficients which minimizes $p-p_{\text{exact}}$ on the given data set (in some sense).
        \end{enumerate}
        \item[$\Rightarrow$] Crucial to find good datasets and minimization procedures!
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Another example}
    \begin{minipage}{.5\textwidth}
        {\small
        \begin{itemize}
            \item {\bf Example:} From the image of an eye, locate the pupil as ellipse.
            \item Ellipses: Determined by centre coordinates, two major axes, tilt angle: {\bf Predict 5 numbers.}
            \item {\bf Ground truth:} We are given a {\bf labeled dataset} of 1000 images \emph{with their ''real'' ellipses} entered by humans.
            \item Getting a dataset can be difficult: Noise, balance, representation.
            \item Deep learning vs.~classical methods.
        \end{itemize}
        }
    \end{minipage}
    \begin{minipage}{.4\textwidth}
        \only<1>{\centering\includegraphics[width=\textwidth]{figures/eye_raw.png}}
        \only<2>{\centering\includegraphics[width=\textwidth]{figures/eye_processed.png}}
    \end{minipage}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{Goals today}
        \begin{enumerate}
            \item Understand the architecture of a Deep Neural Network.
            \item Be able to diagnose most common issues in Neural Networks and to avoid them.
            \item Know the building blocks of Convolutional Neural Networks.
            \item How to use Neural Networks to measure properties hard to quantify.
            \framebreak
            \item Understand the following cartoon:
        \end{enumerate}
        \centering\includegraphics[height=.7\textheight]{figures/machine_learning.png}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mytoc{sec_toc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{General implementation notes}
    \begin{itemize}
        \item {\bf Python} has been established as the main programming language for machine learning tasks.
        \item Libraries are provided for the implementation of well-known algorithms: {\bf SciKit Learn, Tensorflow, Keras, PyTorch} for Python, {\bf Caffe2} for C++.
        \item Python is used for a simple implementation, configuration and training, the backend uses compiled C++ code.
        \item Training is computationally expensive. Speed-up through heavily parallelized implementations, using GPUs (CUDA), and using low-rank tensor approximation.
        \item The material shown in this course is provided on
        \url{https://github.com/fablukm/}
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multilayer Perceptron}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Linear Regression as a Neural Network}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{Linear Regression: Prediction}
    \begin{itemize}
        \item Consider a quantity $y\in \R^m$ which depends on $x\in\R^n$.
        \item We {\bf assume} that $y$ depends on $x$ \emph{linearly or affinely}:
        $$
        y = Wx + b\;,
        \qquad \text{for some } W\in\R^{m\times n} \text{ and } b\in \R^m
        \;.
        $$
        \item However, we do not know the exact \emph{parameters} $W$ and $b$.
        \item Need to \emph{learn} $(n+1)m$ parameters \emph{from data}.
        \item Assume we are given data points $\{(x^1, y^1), (x^2, y^2), \dots, (x^N, y^N)\}$. The data is inaccurate, subject to noise.
        \item {\bf Classical Linear Regression:} Find $W$ and $b$ subject to an optimization task, for example
        $$\min_{W\in \R^{m\times n}, b\in \R^m} \sum_{i=1}^N\left\|y^i-(Wx^i+b)\right\|_2^2$$
        {\small
        \item Two norms involved here: $l^1$ over the data, $l^2$ over the components.
        }
        \framebreak
        \item {\bf Simple constructed example:}
        \input{tikz/linear_regression.tex}
        \framebreak
        \item We reformulate this problem in a neural network language.
        \item There are two steps: {\bf Prediction} and {\bf Learning}.
        \item {\bf Prediciton:} Given a \emph{new} input value $x\not\in\{x^1,\dots,x^N\}$. \\
        If the parameters $W$ and $b$ are known, \emph{predict} the value $y(x)$ to be $y^\ast:=Wx+b$.
        A {\bf prediction} follows the computation graph\\[1em]
        \input{tikz/lr_neuralnet_vec.tikz}
        \item For further purposes, we put the linear combination into a \emph{hidden layer} and define the output to be the output of the hidden layer.
        \framebreak
        \item The same graph, but component-by-component:
        \input{tikz/lr_neuralnet.tikz}

        where
        {\small
        \begin{equation*}
            \begin{split}
                y^\ast_k &:= \sum_{j=1}^n w_{kj}x_j + b_k
                \;, \qquad k=1,\dots, m
                \;,\\
                y_k & := y^\ast_k
                \;, \qquad k=1,\dots, m
                \;,
            \end{split}
        \end{equation*}
        $w_{kj}$ and $b$ are \emph{weights which are determined iteratively to fit the data.}}
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Linear Regression: Training}
    \begin{itemize}
        \item {\bf Loss function: } To \emph{learn} the weights $w_{kj}$, we minimize the {\bf mean squared error}. For one sample $(x^i, y^i)$, it is defined as
        $$
        \mse(x^i, y^i):=\frac{1}{m}\sum_{k=1}^m \left(y^i_k - \left(Wx^i+b\right)_k\right)^2
        \;.
        $$
        \item {\bf Gradient descent:} Find weights $W, b$ such that $\mse$ is minimal. Standard iteration:
        \begin{equation*}
            \begin{split}
                w_{kj; \text{new}} &:= w_{kj; \text{current}} - \learningrate \frac{\partial \mse}{\partial w_{kj}}
                \;,\\
                b_{k; \text{new}} &:= b_{k; \text{current}} - \learningrate \frac{\partial \mse}{\partial b_k}
                \;,
            \end{split}
        \end{equation*}
        with \emph{randomly initialized weights} $w_{kj; \text{start}}$ and $b_{j; \text{start}}$,\\
        with \emph{learning rate} $\learningrate>0$.
        \item Linear regression with loss $\mse$: Derivatives easy to compute.
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Intuition for Gradient Descent}
    \centering
    \only<1>{\includegraphics[width=.7\textwidth]{figures/mountain.jpg}}
    \only<2>{\includegraphics[width=.6\textwidth]{figures/gradientdescent1.png}}
    \only<3>{\includegraphics[width=.7\textwidth]{figures/mountain2.jpg}}
    \only<4>{\includegraphics[width=\textwidth]{figures/gradientdescent2.png}}
    \only<5>{\includegraphics[width=.9\textwidth]{figures/gradientdescent3.eps}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Remarks about Gradient Descent}
        \begin{enumerate}
            \item The performance of the algorithm depends on the choice of a learning rate $\learningrate>0$.
            \begin{itemize}
                \item Large learning rate: Fast descent at first, may not converge.
                \item Small learning rate: Slow convergence overall.
            \end{itemize}
            \item No guarantee to find global minimum for a arbitrary loss functions.
            \item {\bf In practise:} Better minimization schemes, derived from gradient descent: ADAM, Stochastic GD.
            \item {\bf In practise:} A tolerance and a maximal number of iterations should be chosen as well.
            \item {\bf Definition:} (Only) here, we call one iteration one \emph{epoch}.
        \end{enumerate}
    \centering $\boldsymbol{\Rightarrow}$ {\bf Parameters} are learned by minimization of the loss function, but {\bf hyperparameters} are determining the algorithm, and are chosen at the beginning!
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Towards Neural Networks 1: Hidden Layers}
    \begin{minipage}{.5\textwidth}
        \input{tikz/lr_neuralnet_hl.tikz}
    \end{minipage}
    \begin{minipage}{.4\textwidth}
        \begin{itemize}
            \item Change the size of the hidden layer to $m_1\neq m$.
            \item Compute $y_k$ by another linear combination of the $y_l^{\ast}$.
            \item More weights to learn: $W^{[\text{in},\text{hidden}]}\in\R^{m_1\times n}$ and $W^{[\text{hidden}, \text{out}]}\in\R^{m\times m}$.
            \item Gradient descent needs derivatives w.r.~to both weights.
            \item[$\boldsymbol{?}$] {\bf What will be different now?}
        \end{itemize}
    \end{minipage}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Towards Neural Networks 2: Hidden Layers}
    \begin{itemize}
        \item Linear of linear is linear: {\bf Nothing changed.}
        \item Add one more hidden layer with $m_2$ nodes.
        \item Hidden layer 1 \emph{gets propagated} by linear combination to Hidden layer 2.
        \item Introduces weights $W^{[1,2]}\in \R^{m_2\times m_1}$.
        \item[$\boldsymbol{?}$] {\bf What will be different now?}
    \end{itemize}

    \centering \input{tikz/lr_neuralnet_hl2.tikz}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Towards Neural Networks 3: Activation functions}
    \begin{itemize}
        \item {\bf Still nothing has changed.}
        \item {\bf Idea:} Apply a non-linear {\bf Activation Function} $g$ to the output of a hidden layer.
        \item Several activation functions have been proposed for different purposes.
        \item Replace a hidden node $h^{[l]}_{k}$ (the $k$-th node in the $l$-th layer) by
        $$
            h^{[l]}_k := g(h^{[l]}_k)
        $$
    \end{itemize}
    %
    \centering\input{tikz/lr_neuralnet_af.tikz}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multilayer Perceptron: Definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{Multilayer Perceptron: Forward}
        \centering\input{tikz/lr_neuralnet_mlp.tikz}
        \framebreak
        %
        \begin{definition}[Multilayer Perceptron (MLP)]
            A multilayer perceptron (\emph{neural network}) architecture is defined by
            \begin{enumerate}
                \item the input size $m_0$
                \item the number of hidden layers $L$
                \item the size of each hidden layers $\{m_l\}_{l=1,\dots,L}$
                \item the activation function applied to hidden layer $l$ $\{g^{[l]}\}_{l=1,\dots,L}$
                \item the output size $m_{L+1}$
            \end{enumerate}
            To run an input through the MLP, one needs for each layer
            \begin{equation*}
                \begin{split}
                    \text{weights }& W^{[l, l+1]}\in \R^{m_{l+1}\times m_l}
                    \;,\qquad l\in\{0, \dots, L\}
                    \;,\\
                    \text{ biases }& b^{[l, l+1]} \in \R^{m_{l+1}}
                    \;,\qquad\qquad l\in\{0, \dots, L\}
                    \;.
                \end{split}
            \end{equation*}
        \end{definition}
    \framebreak
    \begin{itemize}
        \item Forward step (if the weights are known):
        \begin{enumerate}
            \item We start with an input $x=(x_1,\dots,x_{m_0})\in \R^{m_0}$.
            \item Compute a linear combination of the input to obtain the nodes in the first hidden layer, and apply an activation function $g^{[1]}$: $$h^{[1]}:=g^{[1]}\left(W^{[0,1]}x+b^{[0,1]}\right)\;.$$
            \item For $l=1,\dots,L-1$, use layer $l$ as input for layer $l+1$:
            $$h^{[l+1]}:=g^{[l]}\left(W^{[l,l+1]}h^{[l]}+b^{[l+1]}\right)\;.$$
            \item Compute the output:
            $$y:=g^{[L]}\left(W^{[L,L+1]}h^{[L]}+b^{[L+1]}\right)\;.$$
        \end{enumerate}
        \item A MLP with input $x$ is a composition of maps. \\
        {\bf Notation:} $y=\nn(x)$.
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Activation Functions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{Activation Functions}
    \centering{\large{\bf The $\tanh$ activation function}}

    \begin{itemize}
        \item Consider $g(\eta):=\tanh(\eta):= \frac{e^{\eta}-e^{-\eta}}{e^{\eta}+e^{-\eta}}$.
        \item Derivative: $g'(\eta) = 1-\left(g(\eta)\right)^2 $
        \item $\displaystyle\lim_{\eta\to-\infty} g(\eta) = -1$, $\displaystyle\lim_{\eta\to\infty} g(\eta) = 1$
        \item Used for \emph{Binary Classification}: ``Is it a car or a house?''
    \end{itemize}
    \includegraphics[width=.7\textwidth]{figures/tanh.eps}
    \framebreak

    {\large{\bf The $\relu$ activation function}}

    \begin{itemize}
        \item Consider $g(\eta):=\relu(\eta):= \max\{0, \eta\}$.
        \item Derivative piecewise: $g'(\eta) = \indicator_{(0, \infty]}(\eta)$ for $\eta\neq 0$.
        \item Drops negative values: ``Activates only positive neurons''.
    \end{itemize}
    \centering\includegraphics[width=.7\textwidth]{figures/relu.eps}
    \framebreak

    {\large{\bf The logistic activation function $\sigma$}}

    \begin{itemize}
        \item Consider $g(\eta):=\sigma(\eta):= \frac{1}{1+e^{-\eta}}$.
        \item Derivative piecewise: $g'(\eta) = g(\eta)(1-g(\eta))$.
        \item Binary $0-1$ classification.
    \end{itemize}
    \centering\includegraphics[width=.7\textwidth]{figures/sigmoid.eps}
    \framebreak

    {\large{\bf The Softmax activation function}}

    \begin{itemize}
        \item Consider $g: \R^m\to \R^m$ written as $(g_1, \dots, g_m)$. Define
        $$
            g_k(\eta_1, \dots, \eta_m) := \frac{e^{\eta_k}}{\sum_je^{\eta_j}}
            \;,\quad k=1,\dots,m
            \;,
        $$
        \item Examples:
            \begin{equation*}
                \begin{split}
                    g(0,1,0)&\simeq(0.2, 0.6, 0.2)\\
                    g(0,10,0)&\simeq(0, 1, 0)\\
                    g(0,-10,0)&\simeq(0.5, 0, 0.5)
                \end{split}
            \end{equation*}
        \item ``Scales to $[0,1]$ and discriminates small values.''
        \item $\sum_k g_k=1$: Can be interpreted as a discrete probability over $\eta$.
        \item ``Is it a dog, a cat, a bird, a house, or a car?''
        \item Derivative piecewise: $\frac{\partial g_k}{\partial \eta_j} = g_k(\delta_{kj}-g_j)$ .
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Lab: MLP for digit recognition 1}
    \begin{minipage}{.7\textwidth}
        {\small
        \begin{itemize}
            \item {\bf Classification Task:} Given an image of a handwritten digit, detect the digit on the image.
            \item {\bf Input:} Greyscale image $28\times 28$: $m_0=28^2=784$.
            \item {\bf Output:} $\mathbf{p}=(p_0, \dots, p_9)\in [0,1]^{10}$. $p_k$: probability that $k$ on the image.
            \item Choice of last activation function $g^{[L]}$?
            \item {\bf Architecture:} We choose a MLP with $L=2, m_1=m_2=512$. Clearly, $m_3=10$.
            \item {\bf Activation function:} $g^{[1]}=\relu$.
        \end{itemize}
        }
    \end{minipage}
    \begin{minipage}{.25\textwidth}
        \includegraphics[width=\textwidth]{figures/mnist_data.png}
    \end{minipage}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Loss function for classification}
        {\small
        \begin{itemize}
            \item {\bf Softmax:} Output is a discrete probability distribution.
            \item {\bf Dataset:} $\dataset=\{(x^i, y^i)\}_{i=1,\dots,N}$ with correct labels $y^i_k\in\{0,1\}$
            \item True outputs are either $0$ or $1$ for each digit.
            \item {\bf Cross-entropy loss for one sample:} (set $0\log(0):=0$)
                \begin{equation*}
                    \begin{split}
                        \loss(y_{\text{pred}}, y_{\text{true}}) &=
                            -\sum_{k=0}^9 \log(y_{\text{pred}})y_{\text{true}} \\
                            &\stackrel{\text{binary}}{=}
                            -\sum_{k=0}^9 y_{\text{true}}\log(y_{\text{pred}})
                                + (1-y_{\text{true}}) \log(1-y_{\text{pred}})
                            \;.
                    \end{split}
                \end{equation*}
            \item If true label is $1$ and predicted tends to $0$: $\loss$ explodes.
            \item Avoid numerical instabilities in implementation.
            \item Loss for entire dataset:
            $$
            \loss_{\text{ce}}(\nn; \dataset) :=
                -\frac{1}{N}\sum_{i=1}^N \loss(\nn(x^i), y^i)
                \;.
            $$

        \end{itemize}
        }
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Backpropagation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{Multilayer Perceptron: Learning}
    \begin{itemize}
        \item {\bf Ground truth:} Given a labeled dataset $\dataset=\{(x^i, y^i)\}_{i=1,\dots,N}$.
        \item We call $x^i$ \emph{features} and $y^i$ \emph{labels}.
        \item For theoretical purposes: All parameters in one vector $\boldsymbol{\xi}$, loss dependent on $\boldsymbol{\xi}$.
        \item Need to minimize a {\bf loss function} (``error'') $\loss(\boldsymbol{\xi}; \dataset)$:
        $$
        \boldsymbol{\xi}^\ast = \arg\min_{\boldsymbol{\xi}} \loss(\boldsymbol{\xi}; \dataset)
        \;.
        $$
        \item {\bf Gradient descent} with learning rate $\learningrate>0$:
        $$
            \boldsymbol{\xi}^{\text{new}} := \boldsymbol{\xi}^{\text{current}}
            - \learningrate \nabla_{\boldsymbol{\xi}}J(\boldsymbol{\xi}^{\text{current}};\dataset)
        $$
    \end{itemize}
    \framebreak
    \centering\input{tikz/lr_neuralnet_mlp.tikz}
    \framebreak
    \begin{minipage}{.6\textwidth}
        {\small
        \begin{itemize}
            \item $L=1$, $m_1=m_{2}=1$.
            \item No indices, and $W\in\R^{m_0}$ vector.
            \item {\color{red}Red:} computable. \\
            {\color{blue}Blue:} already computed.
            \item Chain rule:
            \begin{enumerate}
                \item First step (analogous for bias):
                $$
                \frac{\partial\loss}{\partial \hat{h}} =
                {\color{red}
                \frac{\partial\loss}{\partial y} \frac{\partial y}{\partial \hat{h}}}
                =
                {\color{red}\frac{\partial\loss}{\partial y}} {\color{red}g'}
                \;.
                $$
                \item Second step:
                $$
                \frac{\partial\loss}{\partial W_k} =
                {\color{blue}\frac{\partial\loss}{\partial \hat{h}}}
                {\color{red}
                \frac{\partial \hat{h}}{\partial W_k}
                }=
                {\color{blue}\frac{\partial\loss}{\partial \hat{h}}}
                {\color{red}
                x_k
                }
                \;.
                $$
                (analogous for $\frac{\partial}{\partial b}$).
            \end{enumerate}
        \end{itemize}
        }
    \end{minipage}
    \begin{minipage}{.3\textwidth}
        \input{tikz/backprop.tikz}
    \end{minipage}
    \framebreak
    \begin{itemize}
        \item Gradient descent:
        \begin{equation*}
            \begin{split}
                W_k^{\text{new}} &=
                W_k^{\text{current}} -
                \learningrate \frac{\partial\loss}{\partial W_k}\\
                &= W^{\text{current}}_k -
                \learningrate x_k \frac{\partial\loss}{\partial \hat{h}}\\
                &= W^{\text{current}}_k -
                \learningrate x_k\, g'\left(W^{[\text{current}]}x+b^{[\text{current}]}\right)
                \frac{\partial\loss}{\partial y}
            \end{split}
        \end{equation*}
        \item Exercises: Generalise this formula for
        \begin{enumerate}
            \item More than one layer, but still all dimensions $m_l=1$, $l\geq 1$.
            \item Only one layer, but $m_1, m_2\geq 1$.
            \item General case: $L$ layers, dimensions $m_l$ arbitrary.
            \item[$\Rightarrow$] {\bf No difficult exercise!} Chain rule, but careful with indices.
        \end{enumerate}
    \end{itemize}
    \begin{center}
    {\huge {\bf Backpropagation}}
    \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{Datasets}
    \begin{itemize}
        \item In practise: {\bf Obtaining good and large datasets is hard.}
        \item Large online corporations generate enormous datasets and have users label it (``captcha'').
        \item {\bf Croudsourcing:} Humans label a dataset.
        \item Competitions with data sets (\url{https://www.kaggle.com/competitions}).

        \includegraphics[width=.25\textwidth]{figures/recaptcha.png}
        \framebreak
        \item {\bf Training set:} We need data to train a neural network (e.g.~a MLP).
        \item {\bf Test set:} Once the neural network is trained: \emph{Need to evaluate performance} on another labeled dataset!
        \item {\bf Split the dataset:} Split all labeled data into a subset used for training and its complement used for testing.
        \item {\bf Hyperparameters:} Parameters that determine $\nn$: $L, \{m_l\}_l, \learningrate, \dots$
        \item {\bf Validation set:} Actually, keep a small part of the data set on the side to test different hyperparameters.

        \framebreak
        \includegraphics[width=\textwidth]{figures/testtrain.png}
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Lab: MLP for digit recognition 2: Training}
        \begin{enumerate}
            \item Define the architecture of the Neural Network.
            \item Load the dataset: Greyscale images are matrices with entries between $0$ (black) and $1$ (dark)\footnote{Actually, images get saved as an array of integers between $0$ an $255$.}.
            \item Split it into Training and Test set.
            \item Visualize some of the elements.
            \item Train the network.
            \item Run a prediction.
            \item Test the accuracy on the test set.
        \end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Optimisers}
        \begin{itemize}
            \item Gradient descent: Easiest to understand, but outperformed by many derived algorithms.
            \item ADAM: \emph{Adaptive moment estimation}:\\
                \begin{enumerate}
                    \item Each iteration on a \emph{random batch} of the dataset.
                    \item Renders gradient a random quantity.
                    \item Estimates the first and second moments of gradient.
                    \item New hyperparameters related to estimation.
                    \item Finds an adaptive learning rate $\learningrate$ in each step.
                \end{enumerate}
            \item
        \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Remarks}
    \begin{itemize}
        %\item {\bf Forward vs.~Backward:}
        %    \begin{itemize}
        %        \item In a forward step, start with input, follow the graph to compute output.
        %        \item In a Gradient Descent step: Compute gradient with respect to parameters starting from output, moving towards input.
        %    \end{itemize}
        \item ``Multilayer Perceptron'' is ``Feed-forward Neural Network''
        \item Do {\bf one} optimization step on {\bf various} samples in one iteration: {\bf Batch size} as hyperparameter.
        \item {\bf Epoch:} Number of times optimizer runs through entire dataset.
        \item The more complex a Neural Network gets, the more complex it can be to compute the gradient.
        \item {\bf Rademacher's Theorem:} If $g^{[l]}$ is \emph{locally Lipschitz continuous}, then it is (Lebesgue-)almost everywhere differentiable, but the the chain rule may not hold everywhere. Generalisations are being developed\footnote{Berner et.al.~\emph{Towards a regularity theory for ReLU networks - Chain rule and global error estimates}. May 2019. \url{https://arxiv.org/pdf/1905.04992.pdf}}.
        \hfill $\Rightarrow$ Many open questions.
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Practical remarks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overfitting and Underfitting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{Overfitting and Underfitting}
    After training a Neural Network, observe accuracy on training set and on test set.
    \begin{itemize}
        \item If $\nn$ is {\bf inaccurate on the training and test set}
        \begin{itemize}
            \item Model does not capture data (Underfitting).
            \item Model has low variance, high bias.
            \item {\bf Change:} More epochs, bigger training set, more layers, larger layers.
        \end{itemize}
        \item If $\nn$ is {\bf accurate on the training set, inaccurate on test set}
        \begin{itemize}
            \item Model interpolates training data, extrapolates poorly: {\bf Overfitting}
            \item Model has high variance, low bias.
            \item {\bf Change:} Fewer epochs, smaller training set, smaller $\nn$.
        \end{itemize}
    \framebreak
    \includegraphics[width=.9\textwidth]{figures/overunder1.png}
    \framebreak
    \includegraphics[width=.9\textwidth]{figures/overunder2.png}
    \framebreak
    \item {\bf Regularization:}

    \begin{itemize}
        \item Replace $\loss(\nn; \dataset)$ by
        $$
            \loss(\nn; \dataset) + \lambda \operatorname{Reg}(\nn)
            \;,
        $$
        where
        $$
            \operatorname{Reg}(\nn) := \frac{1}{L} \sum_{l=0}^L\left\|W^{[l, l+1]}\right\|^2
            \;.
        $$
        Common to choose the Frobenius Norm.
        \item Penalise large weights $\Rightarrow$ scaled input for activation.
        \item {\bf New hyperparameter} $\lambda$.
        \item {\tt PyTorch:} Parameter {\tt weight\_decay} in {\tt optimizer}.
    \end{itemize}
    \framebreak
    \item {\bf Dropout:}

        \begin{itemize}
            \item {\bf During training}, randomly deactivate some neurons.
            \item Choose a dropout probability $p_d \in [0,1)$.
            \item During training, each neuron will be multiplied by $0$ with probability $p_d$.
            \item ``Decrease the influence of single neurons on the output of $\nn$.''
            \item {\tt PyTorch}: Before training, set {\tt model.eval()} to deactivate dropout.
        \end{itemize}
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{Initialisation}
    \begin{itemize}
        \item Optimisation: $
            \boldsymbol{\xi}^{\text{new}} := \boldsymbol{\xi}^{\text{current}}
            - \learningrate \nabla_{\boldsymbol{\xi}}J(\boldsymbol{\xi}^{\text{current}};\dataset)$
        \item Initial value needed.
        \item What happens if we initialise all weights to $0$?\\

        \input{tikz/lr_neuralnet_mlp_noannot.tikz}
        \framebreak
        \item {\bf Random initialisation:} Draw initial parameters from probability distribution (uniform, normal).
        \item What happens if the initial values are around zero?
        \item What happens if the initial values are large in absolute value?
        \item {\bf Idea:} For $W^{[l, l+1]}$, decrease variance with $m_l$ to ensure that weights get iterated differently.
        \item {\bf Xavier initialisation\footnote{Glorot and Bengio: \emph{Understanding the difficulty of training deep feedforward neural networks.}, 2010.}:} Initialisation of $W^{[l, l+1]}$ by unform distribution around $0$ with standard deviation $\frac{1}{m_{l}}$.\\[.5em]
        Works well with $\tanh$, $\sigma$ activations.
        \item {\bf He initialisation\footnote{He et al.: \emph{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.}, 2015.}:} Initialisation of $W^{[l, l+1]}, b^{[l, l+1]}$ by $\mathcal{N}\left(0, \frac{1}{m_{l}}\right)$.\\
        Works well with $\relu$ and
        $$
            \prelu_{\beta}(x):=\max(-\beta x, x)
            \;,\qquad
            \text{(typically, $0<\beta<<1$)}
            \;.
        $$
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Deep vs.~Shallow}
    \begin{itemize}
        \item Adding more layers vs.~enlarging present layers: Adding more features vs.~combining present features.
        \item \emph{Deep} layers ($h^{[l]}$ for large $l$) learn more complex features.
    \end{itemize}
    \begin{theorem}[Approximation properties of Neural Networks (Sketch)]
        \begin{enumerate}
            \setcounter{enumi}{-1}
            \item Given a function $f\in C^{(0,1)(\Omega)}$ to be approximated by $\nn$, $\Omega\in\R^{m_0}$ compact.
            \item Both shallow (small $L$) and deep (large $L$) NN can compute the best approximation of $f$ in finitely many steps.
            \item Let $M$ be complexity needed to surely attain accuracy $\varepsilon>0$.
                \begin{itemize}
                    \item If $L=1$, need to increase $m_1$ \emph{exponentially} to attain $\varepsilon>0$.
                    \item In the deep case, it is sufficient to increase $L$ at most \emph{linearly}.
                    \item Depends on \emph{complexity} of $f$: ``How many compositions are needed to express $f$ in elementary functions?''
                \end{itemize}
        \end{enumerate}
    \end{theorem}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Recapitulation: Neural Networks}
    \begin{enumerate}
        \item A Neural Network (MLP) $\nn$ contains a forward step (prediction) and a backward step (learning).
        \item {\bf Forward step:} Composition of affine maps and activation functions, using learned parameters.
        \item {\bf Backward step:} Optimization (``learning'') of parameters to minimise a \emph{loss function} $\loss(\nn; \dataset)$.
        \item {\bf Initialisation:} Find ``smart'' way to initialise minimisation.
        \item {\bf Train/Test set:} $\dataset=\dataset_{\text{train}}\dot{\cup} \dataset_{\text{test}}$, usually $\frac{\left|\dataset_{\text{train}}\right|}{\left|\dataset\right|} \in [0.6, 0.9]$.
        \item {\bf Bias-Variance tradeoff:} $\nn$ needs to fit the data \emph{reasonably} well, but still extrapolate on unknown data.
        \item {\bf ``Magic'':} Many heuristic tweaks, not fully understood why they work well.
        \item ``Deep'': $L\simeq$``depth'' of $\nn$. The deeper the network, the more complex the learned features.
    \end{enumerate}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CNNs}
\subsection{Convolutional layers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{Convolutional Layers}
    \begin{itemize}
        \item Before: Each layer is $g^{[l]}$ of a linear combination of $h^{[l-1]}$: {\bf Fully connected layers}.
        \item Introduce new kinds of layers for image processing.
        \item {\bf Motivation:} Visual computing.
        \item {\bf Image input:} $n_x \times n_y$ pixels, $n_c$ channels: $m_0=n_xn_yn_c$!
        \item {\bf Idea:} To obtain a sense of vicinity: Go through each pixel and combine only its surrounding pixels linearly.
        \item To maximize confusion, what we call convolution here is actually a cross-correlation. Difference: Transposition of the filter.
        \framebreak
        \item {\bf Convolution with a filter:}\\

        \includegraphics[width=.8\textwidth]{figures/convlayer.png}

        \item
        This is an example of a {\bf vertical edge detector}: If the resulting number is large, it means that the pixel values to the left and to the right are different.
        \framebreak
        \item Classical Edge detection: Design such filters for each use case (Sobel, Laplace, Scharr, ...)
        \includegraphics[width=.8\textwidth]{figures/sobel.jpg}
        \item {\bf Convolutional layer:} One layer consists of $m_l$ filters. The entries of the filters are \emph{learned}.
        \item Resulting size: Image size $n\times n$, Filter size $f\times f$: Results in output size $(n-f+1)\times (n-f+1)$.
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Padding}
    \begin{itemize}
        \item {\bf Problem:} Each time a filter is applied, image size decreases.
        \item {\bf Padding:} Embed the image inside an outer layer of zeroes.
        \includegraphics[width=.3\textwidth]{figures/padding.png}
        \item Resulting output size with padding $p$, input size $n$, filter size $f$:  $(n+2p-f+1)\times (n+2p-f+1)$.
        \item Enforce input size$=$output size by $p=\frac{f-1}{2}$.
        \item Therefore filter sizes are usually odd numbers.
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Strided convolution}
    \begin{itemize}
        \item So far:
        \begin{enumerate}
            \item Apply a filter to pixel $(i,j)$ in input to get pixel $(k,l)$ in output.
            \item Apply a filter to pixel $(i+1,j)$ in input to get pixel $(k+1,l)$ in output.
        \end{enumerate}
        \item With {\bf stride} $s$:
        \begin{enumerate}
            \item Apply a filter to pixel $(i,j)$ in input to get pixel $(k,l)$ in output.
            \item Apply a filter to pixel $(i+{\color{red}s},j)$ in input to get pixel $(k+1,l)$ in output.
        \end{enumerate}
        \item Output size: $\left\lfloor\frac{n+2p-f}{s}+1\right\rfloor\times \left\lfloor\frac{n+2p-f}{s}+1\right\rfloor$
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Recap: Convolutional Layers}
    \begin{itemize}
        \item A {\bf convolutional layer} applies a filter to an output to detect a feature.
        \item What kind of filter yields the best classification results will be learned.
        \item To ensure more flexibility, a convolutional layer has hyperparameters {\bf Filter size, Padding, Stride}.
        \item Usually followed by $\relu$ activation function.
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{Pooling layer}
    \begin{itemize}
        \item Convolution can result in large output image, features too detailed.
        \item {\bf Idea:} Generate a ``summary'' of the different areas in the image.
        \includegraphics[width=.5\textwidth]{figures/pooling1.png}
        \item Most common: {\bf Max-pooling}.
        \includegraphics[width=.5\textwidth]{figures/pooling2.png}
        \framebreak
        \item Hyperparameters: filter size, stride, kind of pooling.
        \item Introduces more flexibility on output size.
        \item Reduces output to relevant features.
        \item Therefore usually combined with $\relu$.
        \item Considered as part of the convolutional layer.
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{A typical CNN architecture: VGG-16}
    \includegraphics[width=.9\textwidth]{figures/vgg16.png}
    \framebreak
    \begin{itemize}
        \item Developed by the {\bf V}isual {\bf C}omputing {\bf G}roup in Oxford\footnote{Simonyan and Zisserman: \emph{Very Deep Convolutional Networks for Large-Scale Image Recognition.} 2015}
        \item VGG-16 classifies for $1000$ objects on an image.
        \item Image size decreases while number of filters increases.
        \item Backpropagation now involves derivatives w.r.~to the new parameters.
        \item Convolutional Layers learn ``$m_l$'' filters: {\bf Feature maps}
        \input{tikz/cnn1.tikz}
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Style Transfer}
\subsection{Lab: Visualisation of Neurons}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Lab: Visualisation of Neurons}
    \begin{itemize}
        \item Lower layers learn simple features: Edges, curves, repetitive patterns.
        \item Higher layers learn complex features, distinguishing very similar outputs (cat ears, dog ears, eyes,...)
        \item Visualization of the filters contained in one layer:
        \begin{enumerate}
            \item Start with a random image.
            \item Choose a neuron $f_{k}^{[l]}$: Filter $k$ in layer $l$.
            \item {\bf Modify the image to maximize the activation of $f_{k}^{[l]}$}.
            \item Display the resulting image.
        \end{enumerate}
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lab: Style transfer learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]
    \frametitle{Lab: Style transfer learning}
    \begin{itemize}
        \item Given one image $I_s$ as a style template and one image $I_c$ as a content template.
        \item {\bf Goal:} Create an image of the content of $I_c$, but in the style of $I_s$.
        \framebreak
        \item {\bf Style transfer learning:} Use a trained Neural Network to measure \emph{difference in style} and \emph{difference in content}.
        \begin{enumerate}
            \item Load a pre-trained Convolutional Network.
            \item Identify layers and neurons (filters) corresponding to \emph{style} and \emph{content}. \\
            Their output define a content loss and a style loss:
            $$
                \loss_{\text{content}}(I, I_c) = \|\operatorname{content}(I)-\operatorname{content}(I_c)\|^2\;,
            $$
            analogously with style loss.\\
            Total variation loss: Combination of style and content loss, plus regularization.
            \item Start with randomised image and iterate the pixel values to minimize Total variation loss.
        \end{enumerate}
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{Style Transfer: Results}
    \centering
    \only<1>{\includegraphics[width=.9\textwidth]{figures/unicv.jpg}}
    \only<2>{\includegraphics[height=.8\textheight]{figures/kandinsky2.jpg}
            \\{\small \emph{``Heavy Red''}, Kandinskiy 1924}}
    \only<3>{\includegraphics[width=.9\textwidth]{figures/out_unicv_kandinsky.jpg}
    \\{\small \emph{``University of Cape Verde''}, Kandinskiy 2019}}
    \only<4>{\includegraphics[height=.8\textheight]{figures/mindelo.jpg}}
    \only<5>{\includegraphics[width=.8\textwidth]{figures/starrynight.jpg}
            \\{\small \emph{``Starry Night''}, Van Gogh 1889}}
    \only<6>{\includegraphics[width=.8\textwidth]{figures/out_mindelo_starrynight.jpg}
    \\{\small \emph{``Starry Monte Cara''}, Van Gogh 2019}}
    \only<7>{\includegraphics[height=.8\textheight]{figures/weinberg_orig.jpg}}
    \only<8>{\includegraphics[width=.8\textwidth]{figures/kandinsky3.jpg}
            \\{\small \emph{``Bavarian Village''}, Kandinskiy 1908}}
    \only<9>{\includegraphics[width=.8\textwidth]{figures/out_weinberg_kandinsky3.jpg}
    \\{\small \emph{``Lake Biel''}, Kandinskiy 2019}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
    \frametitle{Some things left out}
    \begin{itemize}
        \item {\bf Data augmentation:} How to get more out of a dataset.
        \item {\bf Data normalization:} How to scale the input to increase learning performance.
        \item {\bf Exploding gradients:} Really large $L$ imply instabilities of gradients.
        \item {\bf Image segmentation} vs.~classification.
        \item Recurrent Neural Networks (RNNs), Residual Networks, Generative Adversarial Networks, U-Nets, ...
        \item {\bf RNNs:} If your data is \emph{sequential} (e.g.~Time series, Music, Natural Language), there is a type of neuron that learns to predict the next value: \emph{Long short-term memory cells} (LSTM).
    \end{itemize}
\end{frame}

\end{document}
